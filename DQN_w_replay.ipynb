{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfBbh/OARKVCsaLE4GG/fx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nahianAl/DQN/blob/main/DQN_w_replay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4fyUSeanbGRg"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN():\n",
        "\n",
        "    def __init__(self, n_state, n_action, n_hidden , lr):\n",
        "        \n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "\n",
        "            torch.nn.Linear(n_state, n_hidden),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(n_hidden,n_action)\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "    \n",
        "    \n",
        "    def predict(self,s):\n",
        "        with torch.no_grad(): #why no_grad?\n",
        "            return self.model(torch.Tensor(s))\n",
        "\n",
        "    def update(self, s, y):\n",
        "\n",
        "        pred = self.model(torch.Tensor(s))\n",
        "\n",
        "        loss = self.criterion(pred,torch.Tensor(y))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "_cEikRANbXln"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eps_greedy_policy(neural_net, eps, n_action):\n",
        "\n",
        "    \n",
        "    def policy(state):\n",
        "        \n",
        "        if random.random() > eps:\n",
        "\n",
        "            q_value = neural_net.predict(state)\n",
        "            return torch.argmax(q_value).item()\n",
        "\n",
        "        else:\n",
        "            return random.randint(0, n_action - 1 )\n",
        "    \n",
        "    return policy \n",
        "    \n",
        "        "
      ],
      "metadata": {
        "id": "ZW1atZFMnmOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replay(self, memory, replay_size, gamma):\n",
        "\n",
        "    if len(memory) >= replay_size:\n",
        "\n",
        "        replay_data = random.sample(memory, replay_size)\n",
        "\n",
        "        states = []\n",
        "        td_targets = []\n",
        "\n",
        "        for state, action, next_state,reward, is_done in replay_data:\n",
        "\n",
        "            states.append(state)\n",
        "            \n",
        "            q_values = self.predict(state).tolist()\n",
        "\n",
        "            if is_done:\n",
        "                q_values[action] = reward \n",
        "\n",
        "            else:\n",
        "\n",
        "                q_values_next = self.predict(next_state)\n",
        "                q_values[action] = reward + gamma*torch.max(q_values_next).item()\n",
        "\n",
        "            td_targets.append(q_values)\n",
        "\n",
        "        self.update(states, td_targets)"
      ],
      "metadata": {
        "id": "fGh17e9OyP52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Global \n",
        "\n",
        "env = gym.envs.make(\"MountainCar-v0\")\n",
        "\n",
        "n_state = env.observation_space.shape[0]\n",
        "n_action = env.action_space.n\n",
        "n_hidden = 50\n",
        "lr = 0.001\n",
        "\n",
        "dqn = DQN(n_state, n_action, n_hidden, lr)\n",
        "\n",
        "memory = deque(maxlen = 10000)"
      ],
      "metadata": {
        "id": "EE6Hu-VgxJiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(env , neural_net, episodes, replay_size,\n",
        "               gamma = 1.0,  eps = 0.1, eps_decay = 0.99):\n",
        "\n",
        "    for episode in range(episodes):\n",
        "\n",
        "        policy = eps_greedy_policy(neural_net, eps, n_action)\n",
        "\n",
        "        state = env.reset()\n",
        "        is_done = False\n",
        "\n",
        "        while not is_done:\n",
        "            \n",
        "            action = policy(state)\n",
        "\n",
        "            next_state, reward, is_done, _ = env.step(action)\n",
        "            \n",
        "            total_reward_episode\n",
        "\n",
        "            memory.append((state, action , next_state, \n",
        "                           reward, is_done ))\n",
        "            \n",
        "            if is_done:\n",
        "                break\n",
        "\n",
        "            neural_net.replay(memory, replay_size, gamma)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        \n",
        "        eps = max(eps * eps_decay, 0.01)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gvgl9PbSqb0u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_episode = 600\n",
        "replay_size = 20"
      ],
      "metadata": {
        "id": "UvN1_5ae0-vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_reward_episode = [0] * n_episode\n",
        "train_loop(env, dqn, n_episode, replay_size, gamma = 0.9, eps = 0.3 )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sdFe88W6w9l_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}